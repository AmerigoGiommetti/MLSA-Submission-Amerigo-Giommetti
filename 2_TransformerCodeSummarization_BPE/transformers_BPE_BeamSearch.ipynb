{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0j8Xry_PPhR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ML-Based Python Code Summarization\n",
        "Course: Machine Learning for Software Analysis\n",
        "\n",
        "This project implements a machine learning model that generates\n",
        "natural language summaries from Python code snippets.\n",
        "\n",
        "Author: Amerigo Giommetti\n",
        "Academic Year: 2025/2026\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. Imports and Global Configuration\n",
        "# ============================================================\n",
        "\n",
        "# General imports\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Data manipulation imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ML imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Plotting and evaluation imports\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm as tdqm\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Choosing between RAM and GPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "SEED = 42                       # Seed for reproducibility purposes\n",
        "BATCH_SIZE = 128                # Amount of data processed at same time\n",
        "EMBEDDING_DIM = 512             # Complexity of module representation\n",
        "NUM_EPOCHS = 50                 # Number of time the dataset is processed\n",
        "LEARNING_RATE = 5e-5            # Speed at which the model learns\n",
        "MAX_CODE_LEN = 200              # Max input code length\n",
        "MAX_SUMMARY_LEN = 100           # Max input summaries length\n",
        "MAX_SAMPLES_TRAIN = 100000      # Number of couples code/summary in training dataset\n",
        "MAX_SAMPLES_VALIDATION = 1000   # Number of couples code/summary in validation dataset\n",
        "MAX_SAMPLES_TEST = 500          # Number of couples code/summary in test dataset\n",
        "SRC_VOCAB_SIZE = 10000          # Max amount of different token in BPE toknizer for code\n",
        "TGT_VOCAB_SIZE = 10000          # Max amount of different token in BPE toknizer for summaries"
      ],
      "metadata": {
        "id": "5yJHVURiPT43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. Reproducibility and Utility Functions\n",
        "# ============================================================\n",
        "\n",
        "# Sets same seed for all randomness based processes\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "# Use the seed of 1. Global configuration to set for all other processes\n",
        "set_seed(SEED)\n",
        "\n",
        "# Helper function form input length normalization\n",
        "def pad_sequence(seq, max_len, pad_value=0):\n",
        "    return seq[:max_len] + [pad_value] * max(0, max_len - len(seq))"
      ],
      "metadata": {
        "id": "PosJvn_BPVep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# 3. Dataset Loading code_x_glue (Direct Download from S3/GitHub Source)\n",
        "# ===========================================================================\n",
        "\n",
        "# Import for huggingface dataset loads\n",
        "from datasets import load_dataset\n",
        "\n",
        "def load_data_final():\n",
        "    # Code_x_glue dataset loading\n",
        "    print(\"Loading dataset CodeXGLUE...\")\n",
        "    dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"python\")\n",
        "\n",
        "    # Data gathering using already present split of training, validation and test in code_x_glue\n",
        "    train_raw = dataset['train'].select(range(min(MAX_SAMPLES_TRAIN, len(dataset['train']))))\n",
        "    val_raw = dataset['validation'].select(range(min(MAX_SAMPLES_VALIDATION, len(dataset['validation']))))\n",
        "    test_raw  = dataset['test'].select(range(min(MAX_SAMPLES_TEST, len(dataset['test']))))\n",
        "\n",
        "    # Data shaping into dataframes and minor code cleaning\n",
        "    def to_df(ds):\n",
        "        df = pd.DataFrame({'code': ds['code'], 'summary': ds['docstring']})\n",
        "        df['code'] = df['code'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "        df['summary'] = df['summary'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "        return df\n",
        "\n",
        "    return to_df(train_raw), to_df(val_raw), to_df(test_raw)\n",
        "\n",
        "# Our final variables to be used in the code\n",
        "train_df, val_df, test_df = load_data_final()\n",
        "print(f\"Dataset loaded! Number of rows: {len(train_df) + len(test_df) + len(val_df)}\")"
      ],
      "metadata": {
        "id": "xfu5kwd8PVcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. Vocabulary and Tokenization\n",
        "# ============================================================\n",
        "\n",
        "# Import and installation for tokenizers BPE approach\n",
        "!pip install tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Function to train the tokenizer on sub-words\n",
        "def train_bpe_tokenizer(data, vocab_size):\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "    )\n",
        "    tokenizer.train_from_iterator(data, trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# Training call using only training set\n",
        "print(\"Training BPE Tokenizer...\")\n",
        "code_tokenizer = train_bpe_tokenizer(train_df['code'].tolist(), vocab_size=SRC_VOCAB_SIZE)\n",
        "summary_tokenizer = train_bpe_tokenizer(train_df['summary'].tolist(), vocab_size=TGT_VOCAB_SIZE)\n",
        "\n",
        "# Encoding function\n",
        "def encode_bpe(text, tokenizer, max_len):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    ids = encoded.ids\n",
        "    ids = [1] + ids + [2] # Adding <sos> and <eos>\n",
        "    if len(ids) < max_len:\n",
        "        ids += [0] * (max_len - len(ids)) # Padding\n",
        "    else:\n",
        "        ids = ids[:max_len-1] + [2]       # Truncating\n",
        "    return ids\n",
        "\n",
        "# Generating Tensors for the dataloader\n",
        "X_train = torch.tensor([encode_bpe(c, code_tokenizer, MAX_CODE_LEN) for c in train_df['code']]).to(DEVICE)\n",
        "Y_train = torch.tensor([encode_bpe(s, summary_tokenizer, MAX_SUMMARY_LEN) for s in train_df['summary']]).to(DEVICE)\n",
        "X_val = torch.tensor([encode_bpe(c, code_tokenizer, MAX_CODE_LEN) for c in val_df['code']]).to(DEVICE)\n",
        "Y_val = torch.tensor([encode_bpe(s, summary_tokenizer, MAX_SUMMARY_LEN) for s in val_df['summary']]).to(DEVICE)\n",
        "X_test = torch.tensor([encode_bpe(c, code_tokenizer, MAX_CODE_LEN) for c in test_df['code']]).to(DEVICE)\n",
        "Y_test = torch.tensor([encode_bpe(s, summary_tokenizer, MAX_SUMMARY_LEN) for s in test_df['summary']]).to(DEVICE)"
      ],
      "metadata": {
        "id": "VspGCckMPVaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4.1 Positional encoding\n",
        "# ============================================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_dim, max_len=500):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, emb_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, emb_dim, 2) * (-math.log(10000.0) / emb_dim)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "gSfvA6O9RYFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5. Model Definition\n",
        "# ============================================================\n",
        "\n",
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, EMBEDDING_DIM)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, EMBEDDING_DIM)\n",
        "        self.positional_encoding = PositionalEncoding(EMBEDDING_DIM)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=EMBEDDING_DIM,\n",
        "            nhead=8,\n",
        "            num_encoder_layers=4,\n",
        "            num_decoder_layers=4,\n",
        "            dim_feedforward=4 * EMBEDDING_DIM,\n",
        "            dropout=0.3,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(EMBEDDING_DIM, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None):\n",
        "        src_emb = self.positional_encoding(self.src_embedding(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_embedding(tgt))\n",
        "\n",
        "        # Mask to avoid the model being able to see in the future\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(DEVICE)\n",
        "\n",
        "        out = self.transformer(\n",
        "            src=src_emb,\n",
        "            tgt=tgt_emb,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_padding_mask, # Ignores <pad> in input\n",
        "            tgt_key_padding_mask=tgt_padding_mask  # Ignores <pad> in the target\n",
        "        )\n",
        "\n",
        "        return self.fc_out(out)"
      ],
      "metadata": {
        "id": "cHs_edIPS6RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. Training Loop\n",
        "# ============================================================\n",
        "\n",
        "# Dataloader import\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# We use a dataloader to pass the set in batches otherwise the CPU\n",
        "# would receive all the informations at once and crash\n",
        "\n",
        "# Dataloader for the train\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Dataloader for the validation\n",
        "val_dataset = TensorDataset(X_val, Y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Dataloader for the test\n",
        "test_dataset = TensorDataset(X_test, Y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Variables needed for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Initialization of transformer model, optimizer e loss\n",
        "model = TransformerSeq2Seq(\n",
        "    src_vocab_size=SRC_VOCAB_SIZE,\n",
        "    tgt_vocab_size=TGT_VOCAB_SIZE\n",
        ").to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignores padding\n",
        "# Scheduler reduces Learning rate if the val_loss doesn't decrease\n",
        "# to improve model stability during training\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
        "\n",
        "# Variables needed for early stopping\n",
        "best_val_loss = float('inf')  # Initializing loss at infinite\n",
        "patience = 5                  # Epochs to wait for early stopping\n",
        "counter = 0                   # Counter for early stopping\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- TRAINING PHASE ---\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        # Masks (as seen in model forward)\n",
        "        src_mask = (batch_x == 0)\n",
        "        tgt_pad_mask = (batch_y[:, :-1] == 0)\n",
        "\n",
        "        output = model(batch_x, batch_y[:, :-1], src_padding_mask=src_mask, tgt_padding_mask=tgt_pad_mask)\n",
        "        loss = criterion(output.reshape(-1, output.size(-1)), batch_y[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        # Between loss.backward() and the optimization step we use clipping\n",
        "        # to ensure gradients don't explode rendering the model uncapable of learning\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    # Updating the scheduler based on val_loss\n",
        "    if (epoch > 0): scheduler.step(avg_val_loss)\n",
        "\n",
        "    # --- VALIDATION PHASE ---\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            src_mask = (batch_x == 0)\n",
        "            tgt_pad_mask = (batch_y[:, :-1] == 0)\n",
        "            output = model(batch_x, batch_y[:, :-1], src_padding_mask=src_mask, tgt_padding_mask=tgt_pad_mask)\n",
        "            loss = criterion(output.reshape(-1, output.size(-1)), batch_y[:, 1:].reshape(-1))\n",
        "            epoch_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # --- EARLY STOPPING LOGIC ---\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth') # Saving the best model\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "# Calculating perplexity of each epoch through losses array\n",
        "train_perplexity = [np.exp(l) for l in train_losses]"
      ],
      "metadata": {
        "id": "9gvkcH_zPVU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.1 Training plotting\n",
        "# ============================================================\n",
        "\n",
        "# Epochs (x axis)\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "# Validation perplexity\n",
        "val_perplexity = [np.exp(l) for l in val_losses]\n",
        "\n",
        "# Big plot where we will put 2 subplots\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Training & Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Perplexity\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_perplexity, label=\"Training Perplexity\")\n",
        "plt.plot(epochs, val_perplexity, label=\"Validation Perplexity\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Perplexity\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Polished layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VSkK74lnyB7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.2 Best model loading\n",
        "# ============================================================\n",
        "\n",
        "# Loading the model from the file where the best result with respect to validation are gotten\n",
        "model.load_state_dict(torch.load('best_model.pth'))"
      ],
      "metadata": {
        "id": "b7zZTNotTUmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. Inference / Code Summarization (Using Beam Search)\n",
        "# ============================================================\n",
        "\n",
        "def summarize_code_transformer(model, code_sentence, code_tokenizer, summary_tokenizer, beam_size=3, max_len=MAX_SUMMARY_LEN):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encoding input\n",
        "        src_ids = encode_bpe(code_sentence, code_tokenizer, MAX_CODE_LEN)\n",
        "        src_tensor = torch.tensor(src_ids).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        # Init of Beam Search: (score, sequence)\n",
        "        beams = [(0.0, [1])] # 1 is <sos>\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for score, seq in beams:\n",
        "                if seq[-1] == 2: # If predicted <eos> sequence is already over\n",
        "                    new_beams.append((score, seq))\n",
        "                    continue\n",
        "\n",
        "                tgt_tensor = torch.tensor(seq).unsqueeze(0).to(DEVICE)\n",
        "                output = model(src=src_tensor, tgt=tgt_tensor)\n",
        "\n",
        "                # We take the last token probability and log_softmax it\n",
        "                probs = torch.log_softmax(output[0, -1, :], dim=-1)\n",
        "\n",
        "                # Searching for best prediction of this beam\n",
        "                top_v, top_i = probs.topk(beam_size)\n",
        "\n",
        "                for i in range(beam_size):\n",
        "                    new_beams.append((score + top_v[i].item(), seq + [top_i[i].item()]))\n",
        "\n",
        "            # We only keep the best \"beam_size\"\n",
        "            beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
        "\n",
        "            # If all the beams end in <eos> we exit\n",
        "            if all(seq[-1] == 2 for score, seq in beams):\n",
        "                break\n",
        "\n",
        "        # Keep the highest score sequence\n",
        "        best_seq = beams[0][1]\n",
        "\n",
        "        # Decode ignoring <sos>, <eos>, <pad>\n",
        "        decoded = summary_tokenizer.decode(best_seq, skip_special_tokens=True)\n",
        "        return decoded\n",
        "\n",
        "# Function that runs the number of tests given in input\n",
        "def run_test_samples_transformer(n=5):\n",
        "    print(f\"{'ORIGINAL CODE':<50} | {'REAL SUMMARY':<30} | {'GENERATED SUMMARY'}\")\n",
        "    print(\"-\" * 130)\n",
        "\n",
        "    for _ in range(n):\n",
        "        idx = random.randint(0, len(test_df) - 1)\n",
        "\n",
        "        original_code = test_df.iloc[idx]['code']\n",
        "        real_summary = test_df.iloc[idx]['summary']\n",
        "\n",
        "        generated_summary = summarize_code_transformer(\n",
        "            model,\n",
        "            original_code,\n",
        "            code_tokenizer,\n",
        "            summary_tokenizer,\n",
        "            beam_size=3,\n",
        "            max_len=MAX_SUMMARY_LEN\n",
        "        )\n",
        "\n",
        "        short_code = (original_code[:47] + '...') if len(original_code) > 47 else original_code\n",
        "        short_real = (real_summary[:27] + '...') if len(real_summary) > 27 else real_summary\n",
        "\n",
        "        print(f\"{short_code:<50} | {short_real:<30} | {generated_summary}\")\n",
        "\n",
        "# Running the test on 5 samples from test set\n",
        "run_test_samples_transformer(n=5)"
      ],
      "metadata": {
        "id": "zjCa7ZNtPVRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. Evaluation Metrics\n",
        "# ============================================================\n",
        "\n",
        "!pip install rouge_score\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_transformer_bleu_rouge(model, test_df, code_tokenizer, summary_tokenizer, n_samples):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "\n",
        "    # Selecting random samples from test set\n",
        "    samples = test_df.sample(n=min(n_samples, len(test_df)))\n",
        "\n",
        "    print(f\"Evaluating on {len(samples)} samples...\")\n",
        "\n",
        "    for _, row in samples.iterrows():\n",
        "        reference = row['summary']\n",
        "\n",
        "        # Using function defined in 7. Inference/Code summarization (using Beam Search)\n",
        "        prediction = summarize_code_transformer(\n",
        "            model,\n",
        "            row['code'],\n",
        "            code_tokenizer,\n",
        "            summary_tokenizer,\n",
        "            beam_size=3\n",
        "        )\n",
        "\n",
        "        # For Bleu score calculation we should use same tokenizer of the training\n",
        "        ref_tokens = summary_tokenizer.encode(reference).tokens\n",
        "        pred_tokens = summary_tokenizer.encode(prediction).tokens\n",
        "\n",
        "        # BLEU score\n",
        "        bleu = sentence_bleu(\n",
        "            [ref_tokens],\n",
        "            pred_tokens,\n",
        "            smoothing_function=SmoothingFunction().method1\n",
        "        )\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE-L score\n",
        "        rouge = scorer.score(reference, prediction)\n",
        "        rouge_scores.append(rouge['rougeL'].fmeasure)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Final results:\")\n",
        "    print(f\"AVG BLEU:   {np.mean(bleu_scores):.4f}\")\n",
        "    print(f\"AVG ROUGE-L: {np.mean(rouge_scores):.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Chiamata alla funzione aggiornata\n",
        "evaluate_transformer_bleu_rouge(\n",
        "    model,\n",
        "    test_df,\n",
        "    code_tokenizer,\n",
        "    summary_tokenizer,\n",
        "    n_samples=100\n",
        ")"
      ],
      "metadata": {
        "id": "jOhSQrtaPVMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. Downloading results\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "\n",
        "# For downloading models purposes\n",
        "os.makedirs('progetto_scaricabile/models', exist_ok=True)\n",
        "os.makedirs('progetto_scaricabile/tokenizers', exist_ok=True)\n",
        "\n",
        "# Saves PyTorch model\n",
        "torch.save(model.state_dict(), 'progetto_scaricabile/models/best_model.pth')\n",
        "\n",
        "# Saves tokenizers\n",
        "code_tokenizer.save(\"progetto_scaricabile/tokenizers/code_bpe.json\")\n",
        "summary_tokenizer.save(\"progetto_scaricabile/tokenizers/summary_bpe.json\")\n",
        "\n",
        "print(\"File organizedd in 'progetto_scaricabile'\")\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Creating ZIP archive\n",
        "shutil.make_archive('modello_summarization_completo', 'zip', 'progetto_scaricabile')\n",
        "\n",
        "# Downloading on PC\n",
        "files.download('modello_summarization_completo.zip')"
      ],
      "metadata": {
        "id": "uJw-ICk6gNpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}